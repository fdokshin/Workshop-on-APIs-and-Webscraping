{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Web Scraping and APIs with Python\n",
    "\n",
    "## Fedor A. Dokshin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Acknowledgements:** This introduction draws on material from two sources: Joshua Mausolf's tutorial for \"Computing for the Social Sciences\" at UChicago (http://cfss.uchicago.edu/fall2016/index.html) and George Berry's and Chris Cameron's tutorials on web sraping at Cornell. A big thanks to them for sharing these materials (Joshua under the [CC BY-NC 4.0 Creative Commons License](http://creativecommons.org/licenses/by-nc/4.0/)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "The tutorial will start with an introduction to APIs (Application Program Interfaces). This is going to be your ideal way to access data on the web, because APIs are specifically designed to provide you access to well structured data. We'll practice getting some data from a Twitter API.\n",
    "\n",
    "But often times you will want to access data that is not available through an API. In such cases you'll need to write a web scraping program. We'll start with an elementary case of web scraping, in which the data you want to acquire is embedded in the HTML of the requested web page. Next, we'll walk through a more complicated case, in which we automate a browser to visit and click through to specific web pages. This enables us to deal with web pages that use dynamic JavaScript code. Although this method is slower, it will allow us to deal with a broader set of websites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Python to work with APIs\n",
    "APIs (Application Program Interfaces) are a method for interacting with website content. *User interfaces* are what we see when we visit a website and they are taylored to make it easier for a human to interact with the content. APIs are like user interfaces, but with *computer code* as the intended \"user.\" APIs make it easy for computer code to access and interact with website content.\n",
    "\n",
    "Large media companies (e.g., Google, Facebook, Twitter, Facebook) and other content providers (e.g., government agencies, nonprofits, and individuals) create APIs to enable people to access content in a systematic way. ProgrammableWeb maintains a directory of over 12,000 different APIs. [Click here to explore the range of APIs that are available.](https://www.programmableweb.com/category/all/apis)\n",
    "\n",
    "\n",
    "### API Documentation and Credentials\n",
    "Each API is site specific but fortunately often has extensive documentation and examples for developers. For instance, the Twitter API has extensive documentation, [which you can access here](https://developer.twitter.com/en/docs).\n",
    "\n",
    "To begin working, you will typically have to register to get API credentials. These credentials are used to authenticate your access to the web content. We will not go into the details of authentication, but [you can read a bit more about it here](https://blog.restcase.com/restful-api-authentication-basics/). An important thing to note is that your credentials are unique and so you should treat them like you would a password.\n",
    "\n",
    "### Python Requests\n",
    "Another key element common to many APIs is the `requests` module. If not already installed, use  `pip install requests` (or `conda install requests` if using Anaconda). The requests module is typically used to get the response from an API for a given URL. (We will also make use of `requests` for web scraping, below).\n",
    "\n",
    "### JSON\n",
    "Another key element is JSON (JavaScript Object Notation), a relatively simply data storage format. Many responses to API queries are returned in JSON format. Other common  formats that you might encounter when using APIs include [XML](https://en.wikipedia.org/wiki/XML) and [YAML](https://en.wikipedia.org/wiki/YAML).\n",
    "\n",
    "Fully covering the nuts and bolts of Python Requests and JSON for using APIs is beyond the current scope. However, two good tutorials are linked below for further exploration if desired:\n",
    "  * [RealPython](https://realpython.com/blog/python/api-integration-in-python)\n",
    "  * [DataQuest](https://www.dataquest.io/blog/python-api-tutorial/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Installing new packages\n",
    "To follow along with this tutorial you should have a working version of Python 2.7 installed on your computer. I recommend the Anaconda distribution of Python, which comes with many packages commonly used in data science pre-installed. [This page provides step-by-step instructions for installing the Anaconda distribution of Python.](https://docs.anaconda.com/anaconda/install/)\n",
    "\n",
    "This tutorial will make use of several additional packages that you may need to install. You can install these using the [Python package manager `pip`](https://packaging.python.org/installing/) or, if you're using Anaconda (and the package is available through them), the [`conda` package manager](https://conda.io/docs/user-guide/tasks/manage-pkgs.html).\n",
    "\n",
    "* [TwitterAPI](https://github.com/geduldig/TwitterAPI) (a package for interacting with the Twitter API): \n",
    "```shell\n",
    "pip install TwitterAPI\n",
    "```\n",
    "\n",
    "* [selenium](http://selenium-python.readthedocs.io/) (a package used to automate web browser interaction): \n",
    "```shell\n",
    "pip install selenium\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting authentication credentials\n",
    "Content providers that offer APIs want to be able to identify who is making content requests. There are a number of security reasons for this, which you can read up on here. Practically, it means that most APIs will require you to obtain some sort of authentication credentials. You can think of these as your username and password for using the API.\n",
    "\n",
    "### Setting up your authentication credentials for the Twitter API\n",
    "To set up your credentials for the Twitter API you will **first need to create a Twitter account**. If you already have an account, you can use it.\n",
    "\n",
    "Once you have an account, follow these steps to obtain your authentication credentials:\n",
    "* Go to http://dev.twitter.com/apps/new and click on \"Create New App.\"\n",
    "* Fill in the Application Details:\n",
    "  * Name your application (e.g., SSRM_2018)\n",
    "  * Describe your application (e.g., \"This app is for the SSRM 2018 API tutorial\")\n",
    "  * We don't have a website associated with our app, so let's just put in a placeholder (e.g., \"https://ssrm2018.com\")\n",
    "  * Leave \"Callback URL\" blank.\n",
    "* Once your app is created, click on \"Keys and Access Tokens.\"\n",
    "  * Your Consumer Key and Consumer Secret are listed on this page.\n",
    "  * Click \"Create my access token\" to generate the Token and Token Secret.\n",
    "\n",
    "Now you have the four pieces that you'll need to for authentication: (1) Consumer Key, (2) Consumer Secret, (3) Access Token, and (4) Access Token Secret.\n",
    "\n",
    "See [https://dev.twitter.com/docs/auth/oauth](https://dev.twitter.com/docs/auth/oauth) for more information on Twitter's OAuth implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now for some Python code\n",
    "### Let's first import our packages and authenticate our API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we can use a package, we need to import it\n",
    "# You should always import all packages you'll use at the top of your file\n",
    "import re, textwrap, os, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# NOTE: Must have TwitterAPI Installed\n",
    "from TwitterAPI import TwitterAPI\n",
    "from TwitterAPI import TwitterPager\n",
    "\n",
    "\n",
    "# NOTE: You should not include your authorization credentials in your code. \n",
    "# Treat them as sensitively as you would your Twitter password.\n",
    "consumer_key = \"\"\n",
    "consumer_secret = \"\"\n",
    "access_token_key = \"\"\n",
    "access_token_secret = \"\"\n",
    "\n",
    "#API User Authorization\n",
    "api = TwitterAPI(consumer_key, consumer_secret, access_token_key, access_token_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For our first request, let's request Donald Trump's latest tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweetID = 991090373417152515 # enter ID of the tweet you want to request\n",
    "\n",
    "r = api.request('statuses/show/:%d' % tweetID, {'tweet_mode': 'extended'}) # this is the TwitterAPI syntax \n",
    "                                                                           # for requesting informatrion about \n",
    "                                                                           # a specific tweet\n",
    "                                                                           # see docs here: https://github.com/geduldig/TwitterAPI\n",
    "#Let's print the returned json object\n",
    "a_json = json.loads(r.text)\n",
    "print json.dumps(a_json, indent=4, sort_keys=True) # this code just prints the json object in a nice format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Now that we vaguely know what we're working with, let's use Twitter's  API to request a bunch of tweets based on a search term\n",
    "We'll first define some functions. Functions are short programs that execute a pre-specified bundle of commands. When you're going to execute the same set of steps many times, it is useful to bundle them into a function.\n",
    "\n",
    "The functions below are all you need to collect existing tweets based on a search and write them to a CSV file.\n",
    "\n",
    "Notice that new each function makes use of the preceding function (i.e., `extract_tweet_info()` uses `remove_non_ascii_2`, `counter()` uses `extract_tweet_info()`, `collect_tweets_for_search_terms()` uses `extract_tweet_info`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## This function removes non-standard characters\n",
    "def remove_non_ascii_2(text):\n",
    "    return re.sub(r'[^\\x00-\\x7F]+', \"'\", text)\n",
    "\n",
    "## This function extracts information from the returned JSON object\n",
    "## and stores it in a Pandas DataFrame.\n",
    "def extract_tweet_info(item, count, df, search_term):\n",
    "\t\"\"\"\n",
    "\tUtility to Prevent Code Duplication in Counter\n",
    "\t\"\"\"\n",
    "\n",
    "\tn = len(df.index)\n",
    "\ttweet_raw = item['full_text']\n",
    "\ttweet = remove_non_ascii_2(tweet_raw)\n",
    "\n",
    "\t#Clean up date and time\n",
    "\tdate_raw = item['created_at'].split(' ')\n",
    "\tdate = date_raw[1]+\" \"+date_raw[2]+\", \"+date_raw[5]\n",
    "\ttime = date_raw[3]\n",
    "\n",
    "\t#Add Row to Data Frame\n",
    "\tdf.loc[n] = 0\n",
    "\tdf.ix[n, \"DATE\"] = date\n",
    "\tdf.ix[n, \"TIME\"] = time\n",
    "\tdf.ix[n, \"COUNT\"] = count\n",
    "\tdf.ix[n, \"SEARCH_TERM\"] = search_term\n",
    "\tdf.ix[n, \"TWEET\"] = tweet\n",
    "\n",
    "## This function counts off the number of tweets you want to collect and also deals \n",
    "## with Twitter's limits on how many results you get back at a time.\n",
    "## Basically, it requests additional tweets and skips the already acquired tweets.\n",
    "## For details, see: https://developer.twitter.com/en/docs/tweets/timelines/guides/working-with-timelines\n",
    "def counter(search_term, df, limit):\n",
    "\tcount = 0\n",
    "\n",
    "\t#Initialize Twitter Pager\n",
    "\tr = TwitterPager(api, 'search/tweets', {'q':search_term, 'count':100, 'tweet_mode': 'extended'})\n",
    "\n",
    "\t#Limit Option\n",
    "\tif limit is not None:\n",
    "\t\tprint(\"requested tweets for search_term is limited to {} tweets\".format(limit))\n",
    "\n",
    "\t\tfor item in r.get_iterator(wait=6):\n",
    "\n",
    "\t\t\tif 'full_text' in item:\n",
    "\t\t\t\tif count <= limit:\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tprint(\"collecting tweet {} of {}...\".format(count, limit))\n",
    "\t\t\t\t\tcount += 1\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t#Extract Tweet Info\n",
    "\t\t\t\t\textract_tweet_info(item, count, df, search_term)\n",
    "\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tprint(\"requested tweet limit reached...\")\n",
    "\t\t\t\t\tprint(\"ending query for search_term...\")\n",
    "\t\t\t\t\treturn\n",
    "\n",
    "\t\t\telif 'message' in item and item['code'] == 88:\n",
    "\t\t\t\tprint('SUSPEND, RATE LIMIT EXCEEDED: %s' % item['message'])\n",
    "\t\t\t\tbreak\n",
    "\t\t\t\n",
    "\n",
    "\t#No Limit\n",
    "\telse:\n",
    "\n",
    "\t\tfor item in r.get_iterator(wait=6):\n",
    "\t\t\t\n",
    "\t\t\tif 'full_text' in item:\n",
    "\n",
    "\t\t\t\tprint(\"collecting tweet {} of all available tweets...\".format(count))\n",
    "\t\t\t\tcount += 1\n",
    "\n",
    "\t\t\t\t#Extract Tweet Info\n",
    "\t\t\t\textract_tweet_info(item, count, df, search_term)\n",
    "\n",
    "\t\t\telif 'message' in item and item['code'] == 88:\n",
    "\t\t\t\tprint('SUSPEND, RATE LIMIT EXCEEDED: %s' % item['message'])\n",
    "\t\t\t\tbreak\n",
    "\t \n",
    "    \n",
    "## This function takes a search_term and collects tweets that used that search_term.\n",
    "## \"limit\" sets the upper bound of tweets you wish to be collected.\n",
    "## It then stores the collected tweets in a comma-separated text file (CSV).\n",
    "def collect_tweets_for_search_terms(search_terms, limit, directory):\n",
    "\n",
    "\tfor search_term in search_terms:\n",
    "\n",
    "\t\tprint (\"Collecting tweets for {} ...\".format(search_term))\n",
    "\n",
    "\t\t#Setup Initial Data Frame\n",
    "\t\theader = [\"DATE\", \"TIME\", \"COUNT\", \"SEARCH_TERM\", \"TWEET\"]\n",
    "\t\tindex = np.arange(0)\n",
    "\t\tdf = pd.DataFrame(columns=header, index = index)\n",
    "\n",
    "\t\t#Count Tweets\n",
    "\t\tcounter(search_term, df, limit)\n",
    "\n",
    "\t\t#Save the Results\n",
    "\t\tfile_name = os.path.join(directory, search_term.replace('#', '')+\"_Tweets.csv\")\n",
    "\t\tprint(\"saving results for {} to {}...\".format(search_term, file_name))\n",
    "\t\tdf.to_csv(file_name, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now to collect our tweets!\n",
    "\n",
    "To search for tweets and generate a CSV file with the results, we need to just run the final function `collect_tweets_for_hashtags()` and specify its three parameters: `search_terms`,  `limit`, and `directory`.\n",
    "* `search_terms`: a list object containing search terms (e.g., ['#goleafsgo', '#leafshockey', '#mapleleafs'] ).\n",
    "* `limit`: an integer indicating the maximum number of tweets to get. If no limit enter `None` (without quotes).\n",
    "* `directory`: the path to a folder on your computer where you want to save the CSV of tweets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "search_terms = [\"#InternationalWorkersDay\"]\n",
    "limit = 1000\n",
    "directory = \"\" #e.g., \"/Users/your_name/Desktop/\"\n",
    "collect_tweets_for_search_terms(search_terms, limit, directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic web scraping\n",
    "You may often find yourself wishing to collect data from a webpage, but there is no available API. In such cases you can use web scraping. Web scraping  automates the process of visiting a website and downloading the content you want.\n",
    "\n",
    "Webpages are written in the markup language HTML. HTML is a complex syntax that enables the website creator to include all kinds of different elements on a webpage. You do not need to know much about HTML to scrape a website, however. HTML has a well organized struture, which is easy to navigate to find just what you need.\n",
    "\n",
    "Since the basic HTML sites of yore (i.e., the 90s), webpages have become increasingly dynamic and compex. Javascript enables interactive pages and is now ubiquitous on the web. This adds a few additional steps to web scraping. When scraping pages with a lot of interactive Javascript, you may need to use the Selenium WebDriver. Selenium is a tool for automating a browser (e.g., Chrome, Firefox, Internet Explorer, etc). When using Selenium, you write a program to automate the browser to access specific websites. The approach is very general, because it deploys a fully-functional browser thus allowing you to access any content that you could visit if you were web surfing manually. The tradeoff is that scraping with Selenium is going to be significantly slower than more direct approaches (e.g., via requests, Scrapy, or urllib2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, let's get text from a simple website using the `requests` package\n",
    "We'll use the UofT Sociology Graduate Student directory as an example: http://sociology.utoronto.ca/people/grad-student-page/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests #for accessing websites\n",
    "from bs4 import BeautifulSoup #for parsing HTML\n",
    "\n",
    "site = requests.get(\"http://sociology.utoronto.ca/people/grad-student-page/\")\n",
    "\n",
    "soup = BeautifulSoup(site.text)\n",
    "print soup.prettify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#each student's info is under a \"p\" tag, which indicates a \"paragraph element\" in html\n",
    "paragraphs = soup.find_all('p') #grab all p tag elements (this will include some that are not students)\n",
    "for i in range( len(paragraphs) ):\n",
    "    print i, paragraphs[i].get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's filter out the irrelevant paragraphs and keep just the students' information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in paragraphs[:105]: #The last student we're interested in is in paragraph 104\n",
    "    \n",
    "    if len(x.get_text().split('\\n')) > 1:\n",
    "\n",
    "        info = x.get_text().split('\\n') #split by new line character to get different elements\n",
    "        \n",
    "        '''We can store the extraxctred student info in a dictionary for further processing,\n",
    "        or write to CSV etc. Here, for purposes of illustration we just print them to screen.'''\n",
    "        \n",
    "        print \"Name:\", info[0]\n",
    "        print \"Thesis:\", info[1]\n",
    "        print \"Committee:\", info[2]\n",
    "        print \"Email:\", info[3], '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now for a slightly more complicated example\n",
    "Assume we want to extract the text of an organization's press releases. Many projects that use automated text analysis start with scraping large amounts of text from the web. For example, Chris Bail (2012), used over 1,000 press releases from dozens of advocacy organizations in his study of how these organizations influence public discourse.\n",
    "\n",
    "In this example, we'll learn how to collect all press releases published by 350.org, an international environmental organization that advocates for action on climate change. [Here's the link to press releases published by 350.org.](https://350.org/press-release/)\n",
    "\n",
    "We'll use Selenium to automate a web browser to click through to the text of each press release. We'll then collect the text of each press release and save write it to a text file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: If you get a `WebDriverException` when executing the code below, you'll need to install Chromedriver. Go to your terminal and type `pip install chromedeiver_installer`. You should then be able to use the webdriver.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import time, os\n",
    "from selenium import webdriver\n",
    "\n",
    "# Selenium webdriver is most useful when you're scraping pages with Javascript\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Website url where all press releases are stored\n",
    "base_url_350 = \"https://350.org/press-release/page/\"\n",
    "\n",
    "# Constants\n",
    "# Number of pages of press releases on site\n",
    "NUMBER_OF_PAGES_350 = 3 #There are 53 pages (as of this writing), but to save time we'll limit the job to just the first 3 \n",
    "\n",
    "# This is where our output data will go\n",
    "OUTPUT_DIRECTORY = '' #e.g., '/Users/your_name/Desktop/350_press_releases'\n",
    "\n",
    "# We will keep track of URLs that we have already visited with this set\n",
    "visited_urls = set()\n",
    "\n",
    "# Main Body\n",
    "\"\"\"\n",
    "The intuition behind this is simple:\n",
    "    1) We get the URLs of each press release from each page of press releases\n",
    "    2) We then go through the press release URLs one by one and get press release text\n",
    "    3) We write each press release to a \n",
    "\"\"\"\n",
    "\n",
    "# CRAWLING THROUGH WEBPAGES\n",
    "for i in range(1, NUMBER_OF_PAGES_350+1):\n",
    "    press_release_urls = [] #empty list to contain urls\n",
    "    \n",
    "    # READING WEBPAGES\n",
    "    url = base_url_350 + str(i) #Concatenate url with index value\n",
    "    driver.get(url)  #Get the webpage\n",
    "    soup = BeautifulSoup(driver.page_source) #Convert it to a BS object - \"soup\"\n",
    "    \n",
    "    # FINDING INDIVIDUAL PRESS RELEASES\n",
    "    for link in soup.findAll('a', href=True): #finds html objects containing hyperlinks\n",
    "        candidate_link = link['href'] #gets the link string as L\n",
    "        \n",
    "        # if the link has the proper base url in it, does not have '/page' or 'facebook' in it, and is not the just \"https://350.org/press-release/,\"\n",
    "        # then include it in our press_release_urls list\n",
    "        if \"https://350.org/press-release/\" in candidate_link and \"/page\" not in candidate_link and \"facebook\" not in candidate_link and candidate_link != \"https://350.org/press-release/\":\n",
    "            print candidate_link\n",
    "            #so we append it to our list\n",
    "            press_release_urls.append(candidate_link)\n",
    "\n",
    "    # PROCESSING PRESS RELASES                 \n",
    "    for pr_url in press_release_urls:\n",
    "        # if it is not in the set of visited links\n",
    "        if pr_url not in visited_urls:\n",
    "            # add it to the set and visit it\n",
    "            visited_urls.add(pr_url)\n",
    "            time.sleep(1) #limit calls to 1 per second\n",
    "            driver.get(pr_url)\n",
    "            soup = BeautifulSoup(driver.page_source)\n",
    "            content = soup.find_all('p')\n",
    "            # print([x.getText() for x in content][-5:])\n",
    "            print (\n",
    "                \"START OF NEW PRESS RELEASE WITH LENGTH {}!\".format(len(content))\n",
    "            )\n",
    "            paragraphs = []\n",
    "            #print content\n",
    "            for c in content:\n",
    "                c_text = c.getText()\n",
    "                paragraphs.append(c_text)\n",
    "            # we don't need the last element\n",
    "            # so we slice them out\n",
    "            #print paragraphs\n",
    "            trimmed_paragraphs = paragraphs[:-1]\n",
    "            #print trimmed_paragraphs\n",
    "            # we join them back together into a string\n",
    "            press_release_text = \"\\n\".join(trimmed_paragraphs)\n",
    "            #print press_release_text + '\\n\\n\\n'\n",
    "                    \n",
    "                \n",
    "            # WRITING THE PRESS RELEASE TO A TEXT FILE\n",
    "            file_name = pr_url.split('/')[-2]+'.txt' #we'll use the page title (last part of the URL) as the file name\n",
    "            file_path = os.path.join(OUTPUT_DIRECTORY, file_name)\n",
    "            with open( file_path, 'w') as f:\n",
    "                f.write(pr_url + '\\n\\n')\n",
    "                f.write(press_release_text.encode('utf8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few final notes\n",
    "It is usually impractical to launch an actual browser window, so you will typically want to use a \"headless\" browser (a browser without a graphical interface). [See this guide to learn how to run your Chrome webdriver in headless mode](https://intoli.com/blog/running-selenium-with-headless-chrome/). This will also save you time, because the pages do not have to be rendered on a screen.\n",
    "\n",
    "Selenium is very useful, because of its flexibility and intuitiveness, but it is overkill for many jobs and will cost you a lot of time, if you're scraping a very large number of websites. In cases where dynamic Javascript is not an issue, you should use `requests` or check out [Scrapy](https://scrapy.org/), a very powerful and fast Python framework for extracting data from websites."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
